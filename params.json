{"body":"# CloudPipe\r\n\r\nSimple Buffer that automatically upload it data to S3 on chunks.\r\n\r\n## About\r\n\r\nCloud-Pipe is an easy way of handling huge datas in very low memory and ephemeral writeable filesystems.  \r\nIt can receive data and buffer it to some size specified by you, after fill buffer up, it'll stop receiving data and uploading it to S3 with MultipartUpload. This allows you to upload multiple chunks into S3 and join then later on cloud.\r\n\r\n## Requirements\r\n\r\n- [npm](https://github.com/isaacs/npm)\r\n- [JSss](https://github.com/TotenDev/JSss)\r\n\r\n## Installation\r\n\r\nDownload and install dependencies\r\n\r\n    $ npm install\r\n\r\n## Usage\r\n\r\n\tvar maxChunkSize = 10000000; //10 mb\r\n\tvar cp = require(\"./../src/cloud-pipe.js\")(\"my bucket name\",\"myAccessKey\",\"mySecret\",\"file.zip\",maxChunkSize);\r\n\tcp.on(\"cp-error\",function (err) {\r\n\t\tconsole.log(\"error\",err);\r\n\t});\r\n\tcp.on(\"cp-end\",function () {\r\n\t\tconsole.log(\"end\");\r\n\t});\r\n\tcp.on(\"cp-drained\",function () {\r\n\t\tconsole.log(\"should resume writing\");\r\n\t});\r\n\t//Must be registered in order to CloudPipe get ready\r\n\tcp.on(\"cp-ready\",function () {\r\n\t\tvar chunk = \"OMG\"\r\n        if (!cp.write(new Buffer(chunk,'binary'))) { /*wait until cp-drained event, and write again there*/ }\r\n\t    else { cp.finish(); }\r\n\t});\r\n\r\nMore samples at `samples/` directory.\r\n\r\n## Methods\r\n\r\n#### Initialize Wrapper\r\n\r\nParameters:\r\n\r\n* bucketID - **Type:**string - **Description:**Name of Object in S3 bucket   - **REQUIRED**\r\n* AWSAccessKeyID - **Type:**string - **Description:**AWS AccessKeyID - **REQUIRED**\r\n* AWSSecretAccessKey - **Type:**string - **Description:**AWS SecretAccessKey - **REQUIRED**\r\n* fileName - **Type:**string - **Description:**fileName to be on S3 - **REQUIRED**\r\n* chunkSize - **Type:**integer - **Description:**Chunk in bytes to be on S3 (Got be 5MB or bigger) - **REQUIRED**\r\n* options - **Type:**OptionObject - **Description:**Options Object - **OPTIONAL**\r\n* options.endPoint - **Type:**string - **Description:**End point to be used, default `s3.amazonaws.com` - **OPTIONAL**\r\n* options.useSSL - **Type:**boolean - **Description:**Use SSL or not, default is true - **OPTIONAL**\r\n* options.maxRetry - **Type:**integer - **Description:**Max upload retry, 0 will disable retries. Default is 3 - **OPTIONAL**\r\n\r\nSample:\r\n\r\n    var CloudPipe = require(\"cloud-pipe\")(\"myBucket\",\"AWSAccessKey\",\"AWSSecretAccessKey\",\"fileNameToBeUp\",10000000,{ endPoint:\"secondary.s3.com\",useSSL:false,maxRetry:1 });\r\n---\r\n#### Write Chunk\r\n\r\nThis function will not call error listener, it'll return false if cannot write chunk size.  \r\nAfter it'll fire `cp-drained` event when can write again.\r\n\r\nParameters:\r\n- chunkData - **Type:**Buffer - **Description:**Chunk to be on buffer and after uploaded to S3. (Got be `Buffer` object encoded as `binary`) - **REQUIRED**\r\n\r\nSample:\r\n\r\n    CloudPipe.write(new Buffer('OMG This is my chunk','binary'));\r\n---\r\n#### Finish Upload\r\nThis method will finish upload and upload what remains on `Buffer`. It can take a bit long for large files, since amazon will only answer the request when all parts are joined.\r\n\r\nSample:\r\n\r\n    CloudPipe.finish();\r\n\r\n---\r\n#### Abort Upload\r\nThis method will cancel upload, and delete all uploaded chunks.\r\n\r\nSample:\r\n\r\n    CloudPipe.abort();\r\n\r\n\r\n## Events\r\n\r\n####Ready \r\nThis event **MUST** be registered in order to wrapper start. When this event is reached you are able to start writing.\r\n\r\nEvent-String: `cp-ready`\r\n\r\nSample:\r\n\r\n    //Must be registered to CloudPipe API start\r\n\tcp.on(\"cp-ready\",function () {\r\n\t\tconsole.log(\"I'm ready :)\");\r\n\t}\r\n---\r\n####Drained Notice\r\nThis event will be reached when old buffer has been uploaded with success, so you can start writing again.  \r\nBy default if error happen it will try 3 times until emit error event, this can be set on initialization options.\r\n\r\nEvent-String: `cp-drained`\r\n\r\nSample:\r\n\r\n    cp.on(\"cp-drained\",function () {\r\n\t\tconsole.log(\"resuming cpipe write\");\r\n\t\tcp.write(new Buffer(\"resume data\",'binary'));\r\n\t});\r\n---\r\n####Error\r\nThis event will be reached when an error occur in any part that cannot be recovered, so you need to try again. :(  \r\nDo **NOT** call `terminate` or `abort` method from error event, since those methods can emit an error event.\r\n\r\nEvent-String: `cp-error`\r\n\r\nSample:\r\n\r\n    cp.on(\"cp-error\",function (err) {\r\n\t    console.log(\"error in cpipe\",err);\r\n    });\r\n---\r\n####End\r\nThis event will be reached after finished by `abort()` or `finish()` OR if it didn't start properly.\r\n\r\nEvent-String: `cp-end`\r\n\r\nSample:\r\n\r\n\tcp.on(\"cp-end\",function () {\r\n\t\tconsole.log(\"Bye cpipe\");\r\n\t}\r\n\r\n## Contributing\r\n\r\n1. Fork it\r\n2. Create your feature branch (`git checkout -b my-new-feature`)\r\n3. Commit your changes (`git commit -am 'Added some feature'`)\r\n4. Push to the branch (`git push origin my-new-feature`)\r\n5. Create new Pull Request\r\n\r\n## License\r\n\r\n[GPL v3](Cloud-Pipe/raw/master/LICENSE)","note":"Don't delete this file! It's used internally to help with page regeneration.","google":"UA-33640421-1","tagline":"","name":"Cloud-pipe"}